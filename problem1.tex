\Section{Proposed Research}
\label{sec:proposed-research}

To the end of our comprehensive framework to sense a social network from visual information, our research agenda on social visual analytics spans two specific major aspects: 1) Detection and recognition of social interaction categories; 2) Social network estimation from multiple visual sources. To facilitate our research activity, we have built up a hardware and software infrastructure as a prototype system and achieved promising preliminary results.

\subsection{Detection and recognition of social interaction categories}
\label{sec:activity}

Given a few or many images or videos capturing humans, their societies, and their activities, what should we extract or distill as our first step of the social sensor pipeline? Undoubtedly, many low-level visual cues that exist in images embed rich information about the ties or relationships among the human beings in them. Let us begin with this simple and thought-provoking example, as shown in Fig. \ref{fig:prob1} where two faces are detected from an image using methods such as \cite{ViolaJones} (a-1), and we expect to reveal whether the two individuals are friends, family members, or workmates. Immediately from the two detections we may compute the relative positions of the two faces (a-2), which however has little implication about the social relationship. After we go on to extract other visual descriptions such as expressions (a-3) using methods introduced in \cite{delaTorre:expression},  mutual gazes (a-4) using methods summarized in \cite{Hanson}, and mutual poses (a-5) using methods such as \cite{poselet} or \cite{pose_part}, we tend to believe the two individuals are not workmates. Moreover, we may even recognize the background using scene analysis approach such as \cite{scene} by which we become confident of a `friend' relationship between the two, and finally if given lots of images from which we may accumulate the frequency of their co-occurrences, we are eventually sure that the relationship between the two humans should be more likely classified as the category of `friends'. 

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{intro}
\end{center}
\vspace{-0.25in} \caption{\captionsize 
(a-1) Original picture with detected faces; (a-2) relative positions of the detections; (a-3) Facial expressions; (a-4) Mutual Gazes; (a-5) Mutual poses; (a-6) scene recognition. (b-1) Independently recognizing the two faces: The right face is hard to distinguish between two possible people; (b-2) Recognizing faces using social relationship inferred from (a-1) through (a-6): The right face is now more likely to be associated with one person than the other. \label{fig:prob1}\afterfigspace}
\end{figure}

The example, as well as similar findings \cite{Gallagher,Wang2010,Murillo2012}, is inspiring, in that it convinces us that we have been equipped with sufficient computer vision capabilities for still images: These functionalities provide us a set of \emph{socially informative visual patterns}, and a smart selection and leveraging of them reveals new `words' that has never been seen by computer vision. High-level social contexts sensed from images through these socially informative visual patterns may conversely have immeasurable potentials in pushing traditionally hard problems forward, as has been argued and proved \cite{Stone2008,Stone2010}, for example in the task of face recognition from online imageries: Consider that in recognizing the two faces detected in Fig. \ref{fig:intro} (a-1) the right face is hard to distinguish between `Susan' and `Helen' (see (b-1)). However, the inferred `friends' relationship suggests that the individual should belong to the left person's list of friends (see (b-2)), which includes only `Helen' but not `Susan'.


\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{socialbehavior}
\end{center}
\vspace{-0.25in} \caption{\captionsize 
(a): Socially non-informative co-occurrence of articulations\cite{UTdata}; (b): Socially non-informative collective crowd activities\cite{Choi:context,Choi:recogtrack}; (c): Socially informative salient interactions; (d) Socially informative salient group interactions in the nature; (e) Socially meaningful three-way activity categories defined as `f-formations' by sociology\cite{Kendon1990}.\label{fig:socialbehavior}\afterfigspace}
\end{figure}

Obviously, socially informative visual patterns not only exist in still images, but are also largely available from videos, where human behaviors are observed and recorded. While we may straightforwardly compute and represent the co-occurrences of multiple dynamic expressions, gazes, and poses, it however may not be necessarily true that these dynamic patterns are socially informative. To illustrate this, let us look at  a short video of co-occurrence of two-person actions, such as 'kick', 'punch', etc. (\cite{UTdata}, Fig. \ref{fig:socialbehavior}(a)),  where dynamics of the two people's poses and articulations can be extracted but they have limited extra implications beyond what we can tell from images. Similarly, the socially non-informative behavior patterns also include those group-wise or collective activities of a crowd, within which there are no explicit interactions, such as 'group-walking', 'queueing', etc. (\cite{Choi:context,Choi:recogtrack}, Fig. \ref{fig:socialbehavior}(b)).  Our common sense, on the one hand, implies that socially informative video patterns are more of gestural and conversational interactions, attention-response, as well as turn-taking meetings  (Fig \ref{fig:socialbehavior}(c)). On the other hand, sociology has provided well-defined socialization concepts, in terms of behavior categories, which has been proved in qualitative or quantitative sociological studies to be socially salient and meaningful, whose discovery however has not yet been automated by computer vision. Examples are in Fig. \ref{fig:socialbehavior} (e), which shows diagrams for such socially informative interaction categories, namely `f-formations', in three-way interactions that have been under the investigation of sociology.  


We seek to develop new approaches for detecting, localizing, and recognizing these socially informative visual patterns, i.e., interactions, from videos, in complementary to those companion efforts for still images. Through this effort, we foresee a richer set of heterogenous socially informative visual cues from videos in addition to those from images, so as to enable high-level reasoning as to be introduced in Section \ref{sec:vis2net}. We propose a paradigm as to be detailed in Section \ref{subsec:activity}, which is different from the existing work in three aspects. First, our approach is targeted to interactions that are defined and categories by dictionaries from sociology, but also generic to accommodate arbitrarily specified categories of interest and categories learned from data (Section \ref{sec:actlearn}), as well as interactions in social populations in the nature (Fig. \ref{fig:socialbehavior}(d)). Second, we builds upon realistic videos `in the wild', where the social interactions occur in unconstrained environment in the format of long-term surveillance videos of public areas, with irrelevant human beings co-existing with socially engaged actors as well as scene/background clutter. Our approach will be one that can effectively and efficiently localize and retrieve salient social interactions in space and time from these large volumes of cluttered image sequences. Third, realistic visual materials such as surveillance videos and web videos are frequently in low and varying frame-rate, in low resolution, blurry, as well as in adverse view-points. Our effort will be handling all these realistic conditions and providing robust and practical tools that process more than those simulated, high-quality, manually pre-processed data \cite{UTdata,Choi:context,Choi:recogtrack}.


%Second, we propose to predict activities under social contexts. In this case, social semantics assist in visual understanding in the way that they serve as contextual information for restricting the more possible categories of activities that may occur between a specific group of individuals. This effort is also complementary to the usage of social contexts for identifying the targets presented in the previous section. Consider a simple illustrative example in which we would like to label a conversational scene in a movie as either a `negotiation‘ or a 'debate'. It is possible, in this case, that by the analysis of facial expressions, gestures, and poses, we still have difficulty in distinguishing the two. However, by appropriate face recognition in association with other metadata of the movie, probably with the help of social contexts as introduced in the previous section, we may gain solid confidence regarding the social relationship between the speakers as either `cooperative' or 'adverse'. A cooperative relationship are more likely to imply a negotiating activity, and an adverse relationship implies otherwise. A mechanism, similar to and in companion to the CRF formulation but adapted to video analysis, will be particularly useful.


\subsubsection{Detection and recognition of interactions via matching}
\label{subsec:activity}

To detect a socially informative interaction and recognize its category, we propose an approach where the the interactions of interest are stored as exemplars in a gallery, each exemplar associated with a category label. As illustrated in Fig. \ref{fig:diagram_dataset}(a), our approach is based on matching. Given an exemplar video of a social interaction involving a small handful ($N$) of agents, we detect and localize instances of similar interactions within the long video of a larger gathering/clutter (i.e., $M\ge N$ people), and recognize their categories by matching the video against all the labeled exemplars and then extracting a category label or ranked list of labels from the resulting top-matches. We represent a social interaction as an ensemble of two types of time-varying descriptors: per-agent descriptors that encode the appearance and/or motion of each agent, and relative pairwise descriptors that encode the appearance and/or motion of each agent relative to another. Matching an exemplar interaction then amounts to searching through space and time for ensembles that are similar in some sense. This approach avoids generating explicit semantic descriptions of social interactions, and it is advantageous when one lacks the vocabulary to precisely describe a class of interactions, or when they cannot easily be broken down according to any pre-defined grammar.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{diagram_dataset}
\end{center}
\vspace{-0.25in} \caption{\captionsize 
(a): The diagram illustration of the proposed approach for social interaction detection and recognition; (b-1)-(b-3): Examples of the three categories for two-person interactions; (c-1)-(c-4): Examples of the four categories for three-person interactions; (d) Example of the four-person interaction.\label{fig:diagram_dataset}\afterfigspace}
\end{figure}

More specifically, assume an input video consisting of $T$ frames. By applying domain-appropriate detection and tracking, assume we obtain $M$ space-time tracks of bounding boxes enclosing $M$ faces or bodies. The $M$ targets are to be compared with an interaction exemplar consisting of $S$ frames and $N$ targets that are correctly detected and tracked and all participating in an coherent interaction. Our representation for the input tracks therefore includes $M\times T$ $d_{I}$-dimensional descriptors $\{\mathbf{f}_{m,t}\}_{m=1,2,\cdots,M, t=1,2,\cdots,T}$, where $\mathbf{f}_{m,t}$ encodes the individual activity of the $m$th target at time $t$, and $M\times (M-1)\times T$ $d_{P}$-dimensional pairwise descriptors $\{\mathbf{g}_{m,m',t}\}_{m,m'=1,2,\cdots,M, m\neq m', t=1,2,\cdots,T}$, where $\mathbf{g}_{m,m',t}$ encodes dynamic visual properties of target $m$ exhibited relative to those of target $m'$ at time $t$. For each exemplar, we have two similar descriptor collections $\{\mathbf{f}^{D}_{n,s}\}_{n=1,2,\cdots,N, s=1, 2,\cdots, S}$ and $\{\mathbf{g}^{D}_{n,n',s}\}_{n,n'=1,2,\cdots,N, n\neq n', s=1, 2,\cdots, S}$. We denote the descriptor ensemble for the input at time $t$ to be $\mathcal{Q}_{t}\triangleq\{\mathbf{f}_{m,t},\mathbf{g}_{m,m',t}\}$, and that for the exemplar at time $s$ as $\mathcal{D}_{s}\triangleq\{\mathbf{f}^{D}_{n,s},\mathbf{g}^{D}_{n,n',s}\} $. The input is then represented as $\mathcal{Q}\triangleq\{\mathcal{Q}_{t}\}_{1\leq t\leq T}$ and the exemplar $\mathcal{D}\triangleq\{\mathcal{D}_{s}\}_{1\leq s\leq S}$.

Our tasks are 1) to identify from the $M$ targets in the input $N$ targets that demonstrate the most similar interactive pattern as the $N$ individuals in the exemplar, and 2) to temporally localize the extent of the interaction within the interval $[1, T]$. To formally describe task 1), consider the fact that if one of the $M$ targets is regarded as a participant in an interaction as exemplified by the exemplar $\mathcal{D}$, its behavior should properly match to the behavior of one of the $N$ individuals in the exemplar $\mathcal{D}$. Therefore, we use a $N\times M$ binary matrix $W=[w_{nm}]\in\{0,1\}^{N\times M}$ to formally represent the participant identification, where $w_{nm}=1$ means that the $n$th exemplar individual is matched to the $m$th input target and $w_{nm}=0$ means unmatched targets. For unambiguous matching, we expect each individual in the exemplar to find its unique partner target in the input. This implies that $\sum_{m}w_{nm}=1, \forall n$ and $\sum_{n}w_{nm}\leq 1, \forall m$, \textit{i.e.}, $W\mathbf{1}=\mathbf{1}$ and $\mathbf{1}^{T}W\leq\mathbf{1}^{T}$. Denote $\mathcal{W}\triangleq\{W\in\{0,1\}^{N\times M}| W\mathbf{1}=\mathbf{1}, \mathbf{1}^{T}W\leq\mathbf{1}^{T}\}$, and our former task is essentially to find $W^{*}\in\mathcal{W}$, which encodes the best matching between the input $\mathcal{Q}$ and the exemplar $\mathcal{D}$. To formally describe task 2) is straightforward: We simply look for the starting time $T_{s}$ and ending time $T_{e}$, $1\le T_{s}<T_{e}\le T$, such that the interactive pattern of input $\mathcal{Q}$ during $[T_{s}, T_{e}]$ demonstrates the best similarity with the exemplar $\mathcal{D}$.

Suppose that there are Mahalonobis-parameterized atomic distances $d_{I}(\mathbf{f}, \mathbf{f}')=(\mathbf{f}-\mathbf{f}')^{T}\Sigma_{I}(\mathbf{f}-\mathbf{f}')$ and $d_{P}(\mathbf{g}, \mathbf{g}')=(\mathbf{g}-\mathbf{g}')^{T}\Sigma_{P}(\mathbf{g}-\mathbf{g}')$ ($\Sigma_{I}\succeq 0$ and $\Sigma_{P}\succeq 0$ are positive semi-definite matrices) to compare descriptors, and accordingly define the `instantaneous' matching between input $\mathcal{Q}_t$ and exemplar $\mathcal{D}_s$ under matrix $W$ can be defined as 
\begin{equation}
\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W)=\sum_{nm}w_{nm}d_{I}(\mathbf{f}_{m,t}, \mathbf{f}^{D}_{n,s})+\!\!\!\!\!\!\sum_{nmn'm'}\!\!\!\!\!w_{nm}w_{n'm'}d_{P}(\mathbf{g}_{m,m',s}, \mathbf{g}^{D}_{n,n',t}).
\end{equation}
Consequently, the optimal instantaneous matching between input $\mathcal{Q}_t$ and exemplar $\mathcal{D}_s$ straightforwardly becomes $W^{t,s}\triangleq\arg\min_{W\in\mathcal{W}}\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W)$, and the ensemble distance between them becomes $D(\mathcal{Q}_{t}, \mathcal{D}_{s})\triangleq\min_{W\in\mathcal{W}}\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W)$. As a result, our approach, as illustrated in Fig. XXX, firstly evaluates all $T\times S$ ensemble distances $D(\mathcal{Q}_{t}, \mathcal{D}_{s})$ together with their 'instantaneous optimal matching' $W^{t,s}$ $\forall 1\le t\le T, 1\le s\le S$. This is followed by a Hough voting procedure to find the best matching $W^{*}$ from all instantaneous optimal matchings $\{W^{t,s}\}$. Finally, we construct from all ensemble distances $\{D(\mathcal{Q}_{t}, \mathcal{D}_{s})\}$a quality function $q(t)$, which achieves a negative value if and only if $t$ falls into the interval where the interaction of interest occurs, and therefore enable an efficient branch-and-bound search estimates the temporal extent $[T_{s}, T_{e}]$.  

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{ROC}
\end{center}
\vspace{-0.25in} \caption{\captionsize 
ROC curves for interaction detection using our proposed approach.\label{fig:dataset_compare}\afterfigspace}
\end{figure}

With the prototype system to be introduced in Section \ref{sec:sys}, we have successfully collected and processed a large-scale classroom behavior database consisting of 100 video clips in total. The classroom is ``interactive'' because at various times throughout the lecture students are invited to engage in ad-hoc group discussions about problems provided by the instructor. The scale of our database is orders of magnitude larger than state-of-the-art computer vision datasets (e.g. those used in \cite{UTdata,Choi:context,Choi:recogtrack}), in the number of individuals (10-50 students per camera), the number of cameras (6), and the amount of time (100 minutes per camera, per recording, equaling over 3,000 minutes in total). Through a combination of face detection and tracking, we obtained noisy tracks for all students in each monocular video, upon which we developed descriptive modules that directly extract descriptors for head pose (as individual per-agent descriptor) and body motion (as pairwise descriptor). We manually identified the participants and start/end times of all two-person, three-person, and four-person interactions, obtaining 254 two-person, 112 three-person, and 16 four-person interactions in total. In consultation with education experts we define interaction categories based on the geometric configurations of the participants: three categories for 2-person interactions (same row; different rows with left agent in front; different rows with right agent in front) and four categories for 3-person interactions. Example images for each category can be found in Fig. \ref{fig:diagram_dataset}(b)(c)(d). The annotated interactions range from a few seconds to tens of seconds in length. Since the raw videos arise from five different hour-long session, and we adopt a leave-one-session-out evaluation scheme in partitioning training/gallery samples (exemplars) from test samples (inputs). As our modules include both pose descriptors and relative motion descriptors, and our approach allows to learn optimal representations (in the form of metric learning (ML), see Section \ref{sec:actlearn}), To evaluate the effectiveness of all these functionalities, we turned off some of components as baselines and compared with the full module with all components on. Fig. \ref{dataset_compare} shows the ROC curves of the detection performance for the two-way, three-way, and four-way interactions. As expected we see that performance improves when more parts of the system turned on, which implies that our proposed approach has achieved promising technical quality.

In our following research, we aim to enrich the descriptions of the social behaviors in the current prototype system, explore more accurate and robust comparison and retrieval mechanisms, and in particular, design novel modules for integrating social contextual annotations to allow our framework to be fully `socially-aware'.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Joint Recognition of Multiple Targets with Social Contexts}
%\label{sec:recognition}
%
%The automatic recognition of targets in image and videos is an central task of computer vision. Recognition of objets is not only essential to any system that seeks to manage visual media based on content, but also necessary for automatically annotating images/videos (`auto-tagging') and indexing them. Recognition of faces is even integral to successfully interpreting images/videos of humans and consequently mining the social semantics in these imagery.
%
%Automatic recognition, since its initial stage until most recently, treats the targets as well as the outputted categories to be independent rom one to the other, which is essentially not the case. In order for automatic recognition to truly succeed, we must exploit the syntactic relationships between targets and between categories and use other contextual information. We in this section propose a scalable computational framework for socially-aware recognition systems that exploit contextual information from online social networks. These online communities provide two significant types of information which, to date, have only been scarcely exploited: 1) significant textual metadata, and 2) social network structure as context. The proposed activity seeks computational foundations for exploiting both.
%
%A typical example of socially-aware face recognition is as follows. A user uploads a small, hundred-image `album' (say)  and we seek to automatically recognize the identities of detected faces in the images. To achieve this goal, we represent the detected faces as nodes in a densely connected, undirected graph. With this representation, recognition consists of estimating a joint labeling of the nodes, which can be formulated as the optimization of an energy-based model. What is unique to our approach is that we seek energy models that combine image information (e.g., classifier output) with contextual information drawn from the user's online social network. Another example is scene recognition, where we seek to jointly label all images with their scene categories in several related online album containing thousands of scene images. Similar examples are more than the two mentioned examples.
%
%The description of an individual target is generally in multiple `views', where a view refers to a specific type of attribute that describes the target. A detected face, for example, can be described by skin color, texture, shape configurations of facial organs, etc.. The description of the contextual network relationship, meanwhile, is also in multiple views. The context between two faces(humans) in Facebook, for example, can be number of their shared friends, the number of their co-occurrence, and their relative poses in images, and so on. As we have mentioned, the social networks are usually partially observed, meaning that not all these views are `visible', and a practically useful recognition system must accommodate the un-observed views for the nodes or the links. Formally,  we can represent  the densely connected, undirected graph as $G$ describing the community of $K$ individuals, where $G\triangleq\{N^{(v_1)},A^{(v_2)},P^{(v_1)}, Q^{(v_2)}\}, v_1=1,2,\cdots,V_1,\hspace{5pt} v_2=1,2,\cdots,V_2$,  $N^{(v_1)}$ is a $K\times 1$ vector describing the individuals in the $v_1$th view, $P^{(v_1)}$ is a $K\times 1$ binary vector describing the visibility for that view, $A^{(v_2)}$ is the $K\times K$ affinity matrix describing the relational context for the $v_2$th view, and $Q^{(v_2)}$ is the $K\times K$ visibility matrix for that view. If $P^{(v_1)}(i)=1$, then $N^{(v_1)}(i)$ is the scalar description of the individual $i$ in the $v_1$th view; otherwise if $P^{(v_1)}(i)=0$ then $N^{(v_1)}(i)$ is a missing number indicating the lack of information of this node in this view. Similar notions apply for the matrices $A$ and $Q$.
%
%With this formulation, a natural computational framework for this socially-aware recognition is a conditional random field (CRF) model~\cite{lafferty2001crf, sutton2007icr, he2004mcr} with a node for each item of interest and an edge connecting many pairs of nodes. In this case, the goal is to infer a joint labeling $\vy=\{y_i\}$ of the relevant categories over all nodes $i$ in the graph. We use the notation $y_i\in{\cal L}=\{l_0\ldots l_M\}$ for the discrete label space of the nodes. (In general, these will vary from node to node, but we use a single set of labels here for notational convenience.) An optimal joint labeling is found by maximizing the conditional density
%\begin{equation} \label{eq:crf}
%{\rm Pr}(\vy|G) = \frac{1}{Z(G)}e^{-E(\vy|G)}
%\end{equation}
%where the partition function $Z(G)$ is a data-dependent normalizing constant and the energy $E(\vy|G)$ consists of sums of unary and pairwise potential functions at the nodes and edges:
%\begin{equation}\label{eq:energy}
%E(\vy|G) =  \sum_{i}  \phi_i (y_i|G) + \sum_{(i,j)} \phi_{ij}(y_i, y_j|G).
%\end{equation}
%
%In the CRF framework, the unary potential functions $\phi_i(y_i|G)$ capture information that is local to each node, and the pairwise potentials $\phi_{ij}(y_i, y_j|G)$ represent the compatibilities of possible label pairs across an edge. Therefore, assuming that the unary potentials depend on individual descriptions and pairwise potentials depend on contextual descriptions, (\ref{eq:energy}) can be decomposed as
%\begin{equation}\label{eq:energy_simple}
%E(\vy|G) =  \sum_{i}  \phi_i (y_i|N,P) + \sum_{(i,j)} \phi_{ij}(y_i, y_j|A,Q).
%\end{equation}
%As part of the proposed activity, we will study the application of this model to practical recognition tasks. Our goal is to answer the following three questions: 1) what types of social network context information improve recognition and by how much?; 2) how should multiple-view of information be weighted and how should missing observation be accounted for?; and 3) how can we apply this model at a practical scale? 
%
%\begin{figure}[t!]
%\begin{center}
%\includegraphics[width=3.5in]{ivw_facescores}
%\end{center}
%\vspace{-0.25in} \caption{\captionsize 
%The performance of a commercial face recognition system on tagged faces harvested from Facebook~\cite{Stone2008}. Given the query face in the upper left, the system has returned the remaining faces as being most similar. Similarity to the query decreases from left to right and from top to bottom, and the ground-truth matches are highlighted with green squares. Due to the variability of this `real-world' dataset, the correct matches are not highly ranked.\label{fig:face-results}\afterfigspace}
%\end{figure}
%
%
%
%\SubSubSection{Preliminary Study}\label{sec:prelim-face-results}
%Evidence for the utility of social network context is provided under simplified assumptions by the work of PI Zickler~\cite{Stone2008,Stone2010}, the former of which received the Best Paper Award at the First IEEE Workshop on Internet Vision. This work reported the results of a preliminary study on the ability of social network context to improve face-based identity recognition using a subset of 2641 labeled faces from Facebook. In this study, we restricted our attention to images with two faces (two-node graphs) so that CRF training and inference could be achieved without approximation. The label space ${\cal L}=\{l_0,\ldots,l_M\}$  consists of a set of possible identities, and we considered a simple set of individual node descriptions and contextual relationship descriptions. Specifically, we assumed that all descriptions in all views are observed and error-free, in which case the potentials in (\ref{eq:energy_simple}) are further expanded as a linear combination of view-specific univariate and bivariate \emph{feature functions} independent of observability variables $P,Q$:
%\begin{eqnarray}
%\phi_i(y_i|N,P) &=& \sum_{v_1} \alpha_{v_1}(N^{(v_1)}) f_{v_1}(y_i, N^{(v_1)})\\
%\phi_{ij}(y_i, y_j|A,Q) &=& \sum_{v_2} \beta_{v_2}(A^{(v_2)}) g_{v_2}(y_i, y_j, A^{(v_2)}).
%\end{eqnarray}
%
%
%\begin{figure}[t!]
%\begin{center}
%\includegraphics[width=5in]{rank_graph_small}\\
%{\footnotesize
%\begin{tabular}{ll}
%\hline\hline\multicolumn{2}{l}{\raisebox{-0.1in}{Univariate Functions $f_{v_1}(y_i, N^{(v_1)})$}} \\
%\hline
%\parbox[t]{1.8in}{\raggedright Face score (\textsf{face})} & \parbox[t]{3.5in}{A distribution of likelihoods over ${\cal L}$ returned by a commercial face recognition system (see Fig.~\ref{fig:face-results})} \vspace{0.03in}\\
%\parbox[t]{1.8in}{\raggedright Photo history (\textsf{hist})} & \parbox[t]{3.5in}{Number of times each individual $l_m$ has been tagged in previous photos posted by the photographer} \vspace{0.07in}\\
%\multicolumn{2}{l}{Bivariate Functions $ g_{v_2}(y_i, y_j, A^{(v_2)})$} \\
%\hline
%\parbox[t]{1.8in}{\raggedright Friendship (\textsf{friend})}& \parbox[t]{3.5in}{Binary indicator that is one iff individuals $l_m$ and $l_n$ are `Facebook friends' } \vspace{0.04in}\\
%\parbox[t]{1.8in}{\raggedright Pairwise co-occurrence (\textsf{pair})}& \parbox[t]{3.5in}{Number of times each pair of individuals $(l_m,l_n)$ has been jointly tagged in previous photos posted by the photographer}\\
%\end{tabular}}
%\end{center}
%\captionspace \caption{\captionsize 
%Face recognition performance as a function of rank threshold for a variety of combinations of feature functions (described bottom). At each rank value $R$, the graph displays the proportion of all test samples for which the correct ground-truth identity label appeared in the top $R$ predictions. Social network context improves recognition, and different sources of context information are complimentary\cite{Stone2008}.\label{fig:ivw-curves}\afterfigspace}
%\end{figure}
%
%
%The results are summarized in Figs.~\ref{fig:face-results} and \ref{fig:ivw-curves}. For each combination of feature functions, we determined model parameters ($\alpha$ and $\beta$) by maximizing the conditional log-likelihood of a training set by gradient ascent. Then, for each left-out test photo, exact marginal probabilities were computed for the test photo's face-nodes, and  the outputs were compared against ground truth. Specifically, the marginal probabilities were used to compute a ranked list at each node, and we measured how often the correct identity label appeared in the top $R$ ranks for a sliding value of $R$ (c.f.~\cite{frvt06}).
%
%A number of observations can be drawn from these results. First, even when using the face score (\textsf{face}) alone (see Fig.~\ref{fig:face-results}) social network context plays a vital role. In our data, over 99\% of tagged faces correspond to `friends' of the photographer, and since the identity of the photographer is known (it is included in the input $N$), we can safely reduce the label space ${\cal L}$ from the 15,752 individuals in the global gallery to the hundreds of friends of the input photographer. Without this restriction, the results in Fig.~\ref{fig:face-results} would degrade remarkably. The results in Fig.~\ref{fig:ivw-curves} also demonstrate that social network context substantially improves baseline face recognition, and that different sources of context information are complementary.
%
%
%These preliminary results are encouraging, especially since the network context information that was employed barely scratches the surface of possibilities. Existing work shows that clothing can provide useful information within an album~\cite{anguelov2007cir, zhang2003aah,  song2006cah, sivic2006fpr}, and it is possible that clothing could be used more globally as well, since the long-term clothing trends of certain individuals may be distinguishable from those of others. Recognition of facial attributes~\cite{LNCS53050340}, such as glasses or facial hair, can be used in conjunction with identity scores to improve recognition, and the same is true for gender recognition, which in many online communities is a knowable piece of information. In addition, people associate with each other in homes, schools, outdoors, workplaces, clubs, and so on, and a more explicit representation of these would be beneficial. Certain individuals will appear more often in certain types of scenes, and these `likely' scene types will change depending on who they are with. Temporal and geographic information, when available, may be further predictive of which individual is likely to occur and with whom. 
%
%
%
%In our proposed research, we aim to both qualitatively and quantitatively analyze large collections of images and their embedding network to discover the utility of these multi-view context information, and in particular, explore effective approaches to account for incompletely observed graph, which is the crucial to allow our framework to succeed in realistic network conditions.